{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexerError at position 205\n",
      "[\"'DEF'\", \"'VARIABEL'\", \"'KURUNG_BUKA'\", \"'VARIABEL'\", \"'KURUNG_TUTUP'\", \"'COLON'\"]\n",
      "True\n",
      "[\"'ELIF'\", \"'VARIABEL'\", \"'COMPARISON'\", \"'NUMBER'\", \"'COLON'\"]\n",
      "False\n",
      "Error in line 2\n",
      "[\"'RETURN'\", \"'NUMBER'\"]\n",
      "True\n",
      "[\"'ELIF'\", \"'VARIABEL'\", \"'COMPARISON'\", \"'NUMBER'\", \"'COLON'\"]\n",
      "False\n",
      "Error in line 4\n",
      "[\"'IF'\", \"'TRUE'\", \"'COLON'\"]\n",
      "False\n",
      "Error in line 5\n",
      "[\"'RETURN'\", \"'NUMBER'\"]\n",
      "True\n",
      "[\"'ELSE'\", \"'COLON'\"]\n",
      "True\n",
      "[\"'RETURN'\", \"'NUMBER'\"]\n",
      "True\n",
      "[\"'ELIF'\", \"'VARIABEL'\", \"'COMPARISON'\", \"'NUMBER'\", \"'COLON'\"]\n",
      "False\n",
      "Error in line 9\n",
      "[\"'RETURN'\", \"'NUMBER'\"]\n",
      "True\n",
      "[\"'ELSE'\", \"'COLON'\"]\n",
      "True\n",
      "[\"'RETURN'\"]\n",
      "False\n",
      "Error in line 12\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# lexer.py\n",
    "#\n",
    "# A generic regex-based Lexer/tokenizer tool.\n",
    "# See the if __main__ section in the bottom for an example.\n",
    "#\n",
    "# Eli Bendersky (eliben@gmail.com)\n",
    "# This code is in the public domain\n",
    "# Last modified: August 2010\n",
    "#-------------------------------------------------------------------------------\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "# import sys\n",
    "from cyk_parser import cyk_parser\n",
    "# import argparse\n",
    "from cnf_to_dict import cnf_file_to_dict, cnf_file_to_dict_indra\n",
    "\n",
    "class Token(object):\n",
    "    \"\"\" A simple Token structure.\n",
    "        Contains the token type, value and position.\n",
    "    \"\"\"\n",
    "    def __init__(self, type, val, pos):\n",
    "        self.type = type\n",
    "\n",
    "    def __str__(self):\n",
    "        return '%s' % (self.type)\n",
    "\n",
    "\n",
    "class LexerError(Exception):\n",
    "    \"\"\" Lexer error exception.\n",
    "        pos:\n",
    "            Position in the input line where the error occurred.\n",
    "    \"\"\"\n",
    "    def __init__(self, pos):\n",
    "        self.pos = pos\n",
    "\n",
    "\n",
    "class Lexer(object):\n",
    "    \"\"\" A simple regex-based lexer/tokenizer.\n",
    "        See below for an example of usage.\n",
    "    \"\"\"\n",
    "    def __init__(self, rules, skip_whitespace=False):\n",
    "        \"\"\" Create a lexer.\n",
    "            rules:\n",
    "                A list of rules. Each rule is a `regex, type`\n",
    "                pair, where `regex` is the regular expression used\n",
    "                to recognize the token and `type` is the type\n",
    "                of the token to return when it's recognized.\n",
    "            skip_whitespace:\n",
    "                If True, whitespace (\\s+) will be skipped and not\n",
    "                reported by the lexer. Otherwise, you have to\n",
    "                specify your rules for whitespace, or it will be\n",
    "                flagged as an error.\n",
    "        \"\"\"\n",
    "        # All the regexes are concatenated into a single one\n",
    "        # with named groups. Since the group names must be valid\n",
    "        # Python identifiers, but the token types used by the\n",
    "        # user are arbitrary strings, we auto-generate the group\n",
    "        # names and map them to token types.\n",
    "        #\n",
    "        idx = 1\n",
    "        regex_parts = []\n",
    "        self.group_type = {}\n",
    "\n",
    "        for regex, type in rules:\n",
    "            groupname = 'GROUP%s' % idx\n",
    "            regex_parts.append('(?P<%s>%s)' % (groupname, regex))\n",
    "            self.group_type[groupname] = type\n",
    "            idx += 1\n",
    "\n",
    "        self.regex = re.compile('|'.join(regex_parts))\n",
    "        self.skip_whitespace = skip_whitespace\n",
    "        self.re_ws_skip = re.compile('[\\S^\\n]')\n",
    "\n",
    "    def input(self, buf):\n",
    "        \"\"\" Initialize the lexer with a buffer as input.\n",
    "        \"\"\"\n",
    "        self.buf = buf\n",
    "        self.pos = 0\n",
    "\n",
    "    def token(self):\n",
    "        \"\"\" Return the next token (a Token object) found in the\n",
    "            input buffer. None is returned if the end of the\n",
    "            buffer was reached.\n",
    "            In case of a lexing error (the current chunk of the\n",
    "            buffer matches no rule), a LexerError is raised with\n",
    "            the position of the error.\n",
    "        \"\"\"\n",
    "        if self.pos >= len(self.buf):\n",
    "            return None\n",
    "        else:\n",
    "            if self.skip_whitespace:\n",
    "                m = self.re_ws_skip.search(self.buf, self.pos)\n",
    "\n",
    "                if m:\n",
    "                    self.pos = m.start()\n",
    "                else:\n",
    "                    return None\n",
    "\n",
    "            m = self.regex.match(self.buf, self.pos)\n",
    "            if m:\n",
    "                groupname = m.lastgroup\n",
    "                tok_type = self.group_type[groupname]\n",
    "                tok = Token(tok_type, m.group(groupname), self.pos)\n",
    "                self.pos = m.end()\n",
    "                return tok\n",
    "\n",
    "            # if we're here, no rule matched\n",
    "            raise LexerError(self.pos)\n",
    "\n",
    "    def tokens(self):\n",
    "        \"\"\" Returns an iterator to the tokens found in the buffer.\n",
    "        \"\"\"\n",
    "        while 1:\n",
    "            tok = self.token()\n",
    "            if tok is None: break\n",
    "            yield tok\n",
    "\n",
    "def read_files_input (filename) :\n",
    "    with open(filename, \"r\") as f :\n",
    "        return f.read()\n",
    "\n",
    "def write_files_output(name_out) :\n",
    "    with open(name_out, \"w\") as f :\n",
    "        f.write(\"%s\" % output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rules = [\n",
    "    (r'\\n', 'NEWLINE'),\n",
    "    ('\\\\n', 'NEWLINE'),\n",
    "    (r'False', 'FALSE'),\n",
    "    (r'None', 'NONE'),\n",
    "    (r'True', 'TRUE'),\n",
    "    (r'and', 'AND'),\n",
    "    (r'as', 'AS'),\n",
    "    (r'break', 'BREAK'),\n",
    "    (r'class', 'CLASS'),\n",
    "    (r'continue', 'CONTINUE'),\n",
    "    (r'def', 'DEF'),\n",
    "    (r'elif', 'ELIF'),\n",
    "    (r'else', 'ELSE'),\n",
    "    (r'for', 'FOR'),\n",
    "    (r'from', 'FROM'),\n",
    "    (r'if', 'IF'),\n",
    "    (r'import', 'IMPORT'),\n",
    "    (r'in$', 'IN'),\n",
    "    (r'is$', 'IS'),\n",
    "    (r'not', 'NOT'),\n",
    "    (r'or', 'OR'),\n",
    "    (r'pass', 'PASS'),\n",
    "    (r'raise', 'RAISE'),\n",
    "    (r'return', 'RETURN'),\n",
    "    (r'while', 'WHILE'),\n",
    "    (r'with', 'WITH'),\n",
    "    (r'is$', 'COMPARE'),\n",
    "    (r'not', 'COMPARE'),\n",
    "    ('\\d+', 'NUMBER'),\n",
    "    ('[a-zA-Z_]\\w*', 'VARIABEL'),\n",
    "    # COMPARISON\n",
    "    (r'==|!=|>=|<=|>|<|in|not in|is|is not', 'COMPARISON'),\n",
    "    # ASSIGNMENT\n",
    "    ('=', 'ASSIGNMENT'),\n",
    "    (r'\\/\\/=|\\*\\*=|\\+=|\\-=|\\*=|\\/=|\\%=', 'ASSIGNMENT'),\n",
    "    # ARITMATIKA\n",
    "    (r'[+]|[-]|[*]|[/]|[%]|\\/\\/|\\*\\*', 'ARITMATIKA'),\n",
    "    # TANDA BACA\n",
    "    ('[:]', 'COLON'),\n",
    "    ('[.]', 'DOT'),\n",
    "    (',', 'COMMA'),\n",
    "    # KURUNG COMMENT\n",
    "    ('[(]', 'KURUNG_BUKA'),\n",
    "    ('[)]', 'KURUNG_TUTUP'),\n",
    "    ('\\[', 'KURUNG_SIKU_BUKA'),\n",
    "    ('\\]', 'KURUNG_SIKU_TUTUP'),\n",
    "    ('[#]', 'COMMENT'),\n",
    "    ('\\'\\'\\'', 'COMMENT_MULTILINE'),\n",
    "    ('\\'', 'QUOTE'),\n",
    "]\n",
    "\n",
    "    lx = Lexer(rules, skip_whitespace=True)\n",
    "    filename = \"test.py\"\n",
    "\n",
    "    ipt = read_files_input(filename)\n",
    "    lx.input(ipt)\n",
    "\n",
    "    output = ''\n",
    "    try:\n",
    "        for tok in lx.tokens():\n",
    "            if tok == '':\n",
    "                output = output\n",
    "            else:\n",
    "                output += \"'\" + str(tok) + \"'\" + ' '\n",
    "            #print(tok)\n",
    "    except LexerError as err:\n",
    "        print('LexerError at position %s' % err.pos)\n",
    "    output = output.split(\"'NEWLINE'\")\n",
    "    CNF = cnf_file_to_dict(\"cnf_use.txt\")\n",
    "    # pprint(CNF)\n",
    "    i = 1\n",
    "    for x in output:\n",
    "        x = x.strip().split(' ')\n",
    "        print(x)\n",
    "        hasil_cyk = cyk_parser(x, CNF)\n",
    "        print(hasil_cyk[0])\n",
    "        if (hasil_cyk[0] == False):\n",
    "            print(\"Error in line {}\".format(i))\n",
    "        if (i == 5):\n",
    "            df = pd.DataFrame(hasil_cyk[1])\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[IF]</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NULL</td>\n",
       "      <td>[TRUE, BASE, COMMENTIDN]</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>[COLON]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                         1        2\n",
       "0  [IF]                      NULL     NULL\n",
       "1  NULL  [TRUE, BASE, COMMENTIDN]     NULL\n",
       "2  NULL                      NULL  [COLON]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d956f41e9b1fada610fbeb5c7f9a93f2d93433a1b5ed1e95dc24e8dd52c1ec93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
